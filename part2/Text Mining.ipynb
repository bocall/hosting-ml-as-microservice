{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Data Science - Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientException",
     "evalue": "Required configuration setting 'client_id' missing. \nThis setting can be provided in a praw.ini file, as a keyword argument to the `Reddit` class constructor, or as an environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-44227cf90b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Create PRAW user agent so we can use Reddit API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0muser_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Introducing Data Science Book\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReddit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Our list of subreddits we'll draw into out SQLite database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, site_name, requestor_class, requestor_kwargs, **config_settings)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             ):\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequired_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_secret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONFIG_NOT_SET\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             raise ClientException(\n",
      "\u001b[0;31mClientException\u001b[0m: Required configuration setting 'client_id' missing. \nThis setting can be provided in a praw.ini file, as a keyword argument to the `Reddit` class constructor, or as an environment variable."
     ]
    }
   ],
   "source": [
    "# import PRAW and SQLite3 libaries\n",
    "import praw\n",
    "import sqlite3\n",
    "\n",
    "# Set up connection to SQLite database\n",
    "conn = sqlite3.connect('reddit.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# Execute SQL statements to create topics and comments table\n",
    "c.execute('''DROP TABLE IF EXISTS topics''')\n",
    "c.execute('''DROP TABLE IF EXISTS comments''')\n",
    "c.execute('''CREATE TABLE topics \n",
    "             (topicTitle text, topicText text, topicID text, topicCategory text)''')\n",
    "c.execute('''CREATE TABLE comments\n",
    "             (commentText text, commentID text, topicTitle text, topicText text, topicID text, topicCategory text)''')\n",
    "\n",
    "# Create PRAW user agent so we can use Reddit API\n",
    "user_agent = \"Introducing Data Science Book\"\n",
    "r = praw.Reddit(user_agent=user_agent)\n",
    "\n",
    "# Our list of subreddits we'll draw into out SQLite database\n",
    "subreddits = ['datascience', 'gameofthrones']\n",
    "# Maximum number of posts we'll fetch from Reddit per category. Maximum Reddit allows at any single time is also 1000\n",
    "limit = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific fieds of the topic are appended to the list. We only\n",
    "# use the title and text throughout the exercies but the topic ID\n",
    "# would be useful for building your own (bigger) database of topics\n",
    "def prawGetData(limit, subredditName):\n",
    "    # From subreddits, get hottest 1,000 (in our case) topics\n",
    "    topics = r.get_subreddit(subredditName).get_host(limit=limit)\n",
    "    commentInsert = []\n",
    "    topicInser = []\n",
    "    topicNBR = 1\n",
    "    for topic in topics:\n",
    "        if (float(topicNBR)/limit)*100 in xrange(1,100):\n",
    "            # This part is an informative print and not necessary\n",
    "            # for code to work. It only informs you about the\n",
    "            # download progress\n",
    "            print '************ TOPIC:' + str(topic.id) +\n",
    "            ' ************ COMPLETE: ' + str((float(topicNBR)/limit)*100) +\n",
    "            '% ***'\n",
    "        topicNBR += 1\n",
    "        try:\n",
    "            topicInsert.append((topic.title, topic.selfText, topic.id, subredditName))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            for comment in topic.comments:\n",
    "                # Append comments to a list. These are not used in\n",
    "                # the exercise but now you have them for experimentation\n",
    "                commentInsert.append((comment.body, comment.id, \n",
    "                                      topic.title, topic.selfText, \n",
    "                                      topic.id, subredditName))\n",
    "            except:\n",
    "                pass\n",
    "    print '**************************************'\n",
    "    print 'INSERTING DATA INTO SQLITE'\n",
    "    # Insert all topics into SQLite database\n",
    "    c.executemany('INSERT INTO topics VALUES (?,?,?,?)', topicInsert)\n",
    "    print 'INSERTED TOPICS'\n",
    "    # Insert all comments into SQLite database\n",
    "    c.executemany('INSERT INTO comments VALUES (?,?,?,?,?,?)', commentInsert)\n",
    "    print 'INSERTED COMMENTS'\n",
    "    # Commit changes (data insertions) to database\n",
    "    # Without the commit, no data will be inserted\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function is executed for all subreddits we specified earlier\n",
    "for subject in subreddits:\n",
    "    prawGetData(limit=limit,subredditName=subject)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import sqlite3\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "# Download corora we make use of\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Make connection to SQLite database that contains our Reddit data\n",
    "conn = sqlite3.connect('reddit.db')\n",
    "c.conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word filtering and lowercasing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordFilter() function will remove a term from an array of terms\n",
    "def wordFilter(excluded, wordrow):\n",
    "    filtered = [word for word in wordrow if word not in excluded]\n",
    "    return filtered\n",
    "\n",
    "# Stop word variable contains English stop words per default\n",
    "# present in NTLK\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# lowerCaseArray() function transforms any term to its lowercased\n",
    "# version\n",
    "def lowerCaseArray(wordrow):\n",
    "    lowercased = [word.lower() for word in wordrow]\n",
    "    return lowercased\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First data preparation function and execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(sql):\n",
    "    # Create pointer to SQLite data\n",
    "    c.execute(sql)\n",
    "    data = {'wordMatrix': [], 'all_words': []}\n",
    "    # Fetch data row by row\n",
    "    row = c.fetchone()\n",
    "    while row is not None:\n",
    "        # row[0] is title, row[1] is topic text; we turn them into\n",
    "        # a single text blob\n",
    "        wordrow = nltk.tokenize.word_tokenize(row[0] + \" \" + row[1])\n",
    "        wordrow_lowercased = lowerCaseArray(wordrow)\n",
    "        wordrow_nostopwords = wordFilter(stopwords, wordrow_lowercased)\n",
    "        # We'll use data['all_words'] for data exploration\n",
    "        data['all_words'].extend(wordrow_nostopwords)\n",
    "        # data['wordMatrx'] is a matrix comprised of word vectors;\n",
    "        # 1 vector per document\n",
    "        data['wordMatrix'].append(wordrow_nostopwords)\n",
    "        row = c.fetchone()\n",
    "    return data\n",
    "\n",
    "# Our subreddits as defined earlier\n",
    "subreddits = ['datascience', 'gameofthrones']\n",
    "data = {}\n",
    "\n",
    "# Call data processing function for every subreddit\n",
    "for subject in subreddits:\n",
    "    data[subject] = data_processing(sql='''SELECT topicTitle, topicText, topicCategory \n",
    "                                             FROM topics \n",
    "                                            WHERE topicCategory = ''' + \"'\" + subject \"'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the frequency distribution of our terms\n",
    "wordfreqs_cat1 = nltk.FreqDist(data['datascience']['all_words'])\n",
    "plt.hist(wordfreqs_cat1.values(), bins = range(10))\n",
    "plt.show()\n",
    "wordfreqs_cat2 = nltk.FreqDist(data['gameofthrones']['all_words'])\n",
    "plt.hist(wordfreqs_cat2.values(), bins = range(20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots show that a lot of our terms only appear in one document.\n",
    "# Single-occurance terms such as these are called hapaxes. They\n",
    "# add little value to our model and can be removed\n",
    "print wordfreqs_cat1.hapaxes()\n",
    "print wordfreqs_cat2.hapaxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the most frequent words\n",
    "print wordfreqs_cat1.most_common(20)\n",
    "print wordfreqs_cat2.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 revisited - Data preparation adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialzes stemmer from NLTK library\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "def wordStemmer(wordrow):\n",
    "    stemmed = [stemmer.stem(word) for word in wordrow]\n",
    "    return stemmed\n",
    "\n",
    "# Stop words array defines terms to remove/ignore\n",
    "namual_stopwords = [',','.',')','(','m',\"'m\",\"n't\",'e.g',\"'ve\",'s',\n",
    "                   '#','/','``',\"'s\",\"''\",'!','r',']','=','[','&',\n",
    "                   '%','...','1','2','3','4','5','6','7','8','9',\n",
    "                   '10','--',';','-',':',';']\n",
    "\n",
    "def data_processing(sql, manual_stopwords):\n",
    "    # Create pointer to SQLite data\n",
    "    c.execute(sql)\n",
    "    data = {'wordMatrix': [], 'all_words': []}\n",
    "    # Fetch data row by row\n",
    "    row = c.fetchone()\n",
    "    while row is not None:\n",
    "        # row[0] is title, row[1] is topic text; we turn them into\n",
    "        # a single text blob\n",
    "        wordrow = nltk.tokenize.word_tokenize(row[0] + \" \" + row[1])\n",
    "        wordrow_lowercased = lowerCaseArray(wordrow)\n",
    "        wordrow_nostopwords = wordFilter(stopwords, wordrow_lowercased)\n",
    "        \n",
    "        # Remove manually added stopwords from text blob\n",
    "        wordrow_nostopwords = wordFilter(manual_stopwords,wordrow_nostopwords)\n",
    "        wordrow_stemmed = wordStemmer(wordrow_nostopwords)\n",
    "        \n",
    "        # Temporary word list used to remove hapaxes\n",
    "        interWordList.extend(wordrow_stemmed)\n",
    "        # Temporary word matrix; will become final word matrix after hapaxes removal\n",
    "        interWordMatrix.append(wordrow_stemmed)\n",
    "        \n",
    "        # Get new topic\n",
    "        row = c.fetchone()\n",
    "    \n",
    "    # Make frequency distribution of all terms\n",
    "    wordfreqs = nltk.FreqDist(interWordList)\n",
    "    hapaxes = wordfreqs.hapaxes()\n",
    "    # Loop through temporary word matrix\n",
    "    for wordvector in interWordMatrix:\n",
    "        # Remove hapaxes in each word vector\n",
    "        wordvector_nohapaxes = wordFilter(hapaxesmwordvector)\n",
    "        # Append correct word vector to final word matrix\n",
    "        data['wordMatrix'].append(wordrow_nostopwords)\n",
    "        # Extend list of all terms with corrected word vector\n",
    "        data['all_words'].extend(wordrow_nostopwords)\n",
    "        \n",
    "    return data\n",
    "\n",
    "# Our subreddits as defined earlier\n",
    "subreddits = ['datascience', 'gameofthrones']\n",
    "data = {}\n",
    "\n",
    "# Run new data processing function for both subreddit\n",
    "for subject in subreddits:\n",
    "    data[subject] = data_processing(sql='''SELECT topicTitle, topicText, topicCategory \n",
    "                                             FROM topics \n",
    "                                            WHERE topicCategory = ''' + \"'\" + subject \"'\", \n",
    "                                    manual_stopwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final data transformation and data splitting before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout sample will be used to determine the model's flaws by \n",
    "# constructing a confusion matrix\n",
    "holdoutLength = 100\n",
    "\n",
    "# We create a single data set with every word vector tagged as being\n",
    "# either 'datascience' or 'gameofthrones'. We keep part of the data\n",
    "# aside for holdout sample.\n",
    "labeled_data1 = [(word,'datascience') for word in\n",
    "                data['datascience']['wordMatrix'][holdoutLength:]]\n",
    "labeled_data2 = [(word,'gameofthrones') for word in\n",
    "                data['gameofthrones']['wordMatrix'][holdoutLength:]]\n",
    "labeled_data = []\n",
    "labeled_data.extend(labeled_data1)\n",
    "labeled_data.extend(labeled_data1)\n",
    "\n",
    "# Holdout sample is comprised of unlabeled data from two subreddits:\n",
    "# 100 obervations from each data set. The labels are kept in a separate\n",
    "# data set.\n",
    "holdout_data = data['datascience']['wordMatrix'][:holdoutLength]\n",
    "holdout_data.extend(data['gameofthrones']['wordMatrix'][:holdoutLength])\n",
    "holdout_data_labels = ([('datascience')\n",
    "                       for _ in xrange(holdoutLength)] + [('gameofthrones') for _ in\n",
    "                                                         xrange(holdoutLength)])\n",
    "\n",
    "# A list of all unique terms is created to build the bag of words\n",
    "# data we need for training or scoring a model\n",
    "data['datascience']['all_words_dedup'] = list(OrderedDict.fromkeys(data['datascience']['all_words']))\n",
    "data['gameofthrones']['all_words_dedup'] =list(OrderDict.fromkeys(data['gameofthrones']['all_words']))\n",
    "all_words = []\n",
    "all_words.extend(data['datascience']['all_words_dedup'])\n",
    "all_words.extend(data['gameofthrones']['all_words_dedup'])\n",
    "all_words_dedup = list(OrderedDict.fromkeys(all_words))\n",
    "\n",
    "# Data is turned into a binary bag of words format\n",
    "prepared_data = [({word: (word in x[0]) for word in all_words_dedup}, x[1]) for x in labeled_data]\n",
    "prepared_holdout_data = [({word: (word in x[0]) for word in all_words_dedup}) for x in holdout_data]\n",
    "\n",
    "# Data for model training and testing to be shuffled first\n",
    "random.shuffle(prepared_data)\n",
    "# Size of training data will be 75% of total and remaining 25% will\n",
    "# be used for testing model performance\n",
    "train_size = int(len(prepared_data) * 0.75)\n",
    "train = prepared_data[:train_size]\n",
    "test = prepared_data[train_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first test the performance of our Naive Bayes classifier. NLTK\n",
    "# comes with a classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "# With the classifier trained we can use the test data to get a measure\n",
    "# on overall accuracy\n",
    "nltk.classify.accuracy(classifiet, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test it again on the 200 observations holdout sample and this\n",
    "# time create a confusion matrix\n",
    "classified_data = classifier.classify_many(prepared_holdout_data)\n",
    "cm = nltk.ConfusionMatrix(holdout_data_labels, classified_data)\n",
    "print cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at what is uses to determine the categories by digging\n",
    "# into the most informative\n",
    "print(classifier.show_most_informative_features(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train decision tree classifier\n",
    "classifier2 = nltk.DecisionTreeClassifier.train(train)\n",
    "# Tes classifier accuracy\n",
    "nltk.classify.accuracy(classifier2, test)\n",
    "# Attempt to classify holdout data (scoring)\n",
    "classified_data2 = classifier2.classify_many(prepared_holdout_data)\n",
    "# Create confusion matrix based on classification results and actual labels\n",
    "cm = nltk.ConfusionMatrix(holdout_data_labels, classified_data2)\n",
    "# Show confusion matrix\n",
    "print cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On these 200 observations of the holdout sample the decision tree model tends to classify well when the post is about Game of Thrones but fails miserably when confronted with the data science posts. It seems the model has a preference for Game of Thrones, and can you blame it? Let’s have a look at the actual model, even though in this case we’ll use the Naïve Bayes as our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier2.pseudocode(depth=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
